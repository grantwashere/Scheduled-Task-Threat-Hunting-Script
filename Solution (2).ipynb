{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d7b7853",
   "metadata": {},
   "source": [
    "# Filtering Down the Scheduled Task Data Overview\n",
    "\n",
    "**Description of Dataset**\n",
    "- Over 12,000 CSV files in directory\n",
    "- Over 5 million rows of Scheduled Task Data\n",
    "- 28 columns, per row\n",
    "\n",
    "**Objective:**\n",
    "- Filter down the massive _Scheduled Task Dataset_ to a more managable size\n",
    "\n",
    "**Sections:**\n",
    "1. Functions Used\n",
    "2. Pre-Specified File Paths\n",
    "3. Combining CSV Files\n",
    "4. Counting Unique Tasks\n",
    "5. Filtering Out Pre-Specified Tasks\n",
    "6. Output / Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ebd21",
   "metadata": {},
   "source": [
    "=========================================================================================================\n",
    "\n",
    "### __SECTION 1:__ _Functions Used_ ###\n",
    "__Note:__\n",
    "- These functions are used throughout the script\n",
    "- Can easily be repurposed for other projects\n",
    "- Stored in Python as a _Function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced046aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATES A LIST FROM A SINGLE FILE ##\n",
    "def create_list_from_file(file_path):\n",
    "    from csv import reader\n",
    "    file = open(file_path)\n",
    "    read_file = reader(file)\n",
    "    list_of_file = list(read_file)[1:]\n",
    "    return list_of_file\n",
    "\n",
    "## USED TO FILTER THE DATASET FOR TASKS ##\n",
    "def VanillaFilter(TaskName):\n",
    "    for eachtask in VanillaTasks:\n",
    "        if str(eachtask) in TaskName:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "## CREATES A DATAFRAME FROM A DIRECTORY OF CSV FILES ##\n",
    "def dataframe_from_directory(directory_path):\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    files = glob.glob(directory_path)\n",
    "    df = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "## COUNTS THE NUMBER OF INSTANCES FOR EACH UNIQUE SCHEDULED TASKS ##\n",
    "def counting_unique_tasks(directory_path):\n",
    "    def dataframe_from_directory(directory_path):\n",
    "        import glob\n",
    "        import pandas as pd\n",
    "        files = glob.glob(directory_path)\n",
    "        df = pd.concat([pd.read_csv(fp) for fp in files], ignore_index=True)\n",
    "        return df   \n",
    "    unfiltered_data = dataframe_from_directory(directory_path)\n",
    "    data = unfiltered_data[unfiltered_data.TaskName != 'TaskName']    \n",
    "    values = data['TaskName'].value_counts(dropna=False).keys().tolist()\n",
    "    counts = data['TaskName'].value_counts(dropna=False).tolist()\n",
    "    value_dict = dict(zip(values, counts))    \n",
    "    task_num = 0\n",
    "    print('Total Num. of Unique Tasks:', len(value_dict))\n",
    "    for key in value_dict:\n",
    "        task_num += 1\n",
    "        print('Task', task_num, '---', 'Count:', value_dict[key], '---', 'TaskName:', key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a43bfc1",
   "metadata": {},
   "source": [
    "=========================================================================================================\n",
    "\n",
    "### __SECTION 2:__ _Pre-Specified File Paths (INPUT)_ ###\n",
    "__Note:__\n",
    "- These are the _only_ two inputs needed, other than determining what tasks to filter for\n",
    "- This is the _only_ place where the user _must_ interact with the script\n",
    "\n",
    "__Instructions:__ \n",
    "- Paste the filepaths directly between the apostrophes for 'directory_path' and 'file_path'\n",
    "- If the filepath is not working, try adding a lowercase letter 'r' in front of the first apostrophe\n",
    "        - Example: directory_path = r'C:\\Users\\guest\\Desktop\\TEST2\\*.csv'\n",
    "        - Example: file_path = r'C:\\Users\\guest\\Desktop\\VanillaTasks.csv' \n",
    "- Make sure to include '*.csv' at the end of your filepath for directory (see above)\n",
    "    - This ensures that all files ending in .csv are selected\n",
    "- Stored in Python as a _String_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ea61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DIRECTORY OF SCHEDULED TASK DATA ##\n",
    "directory_path = ''\n",
    "\n",
    "## CSV OF TASKS DEEMED 'VANILLA' OR 'SAFE' ##\n",
    "file_path = '' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f10c04",
   "metadata": {},
   "source": [
    "=========================================================================================================\n",
    "\n",
    "### __SECTION 3:__ _Combining CSV Files_ ###\n",
    "__Note:__\n",
    "- Combines all CSV files within the specified directory into one massive dataset\n",
    "- Prints the length of the combined dataset\n",
    "    - Used to check for mistakes at the end of the script\n",
    "- Stored in Python as a _List of Lists_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ad5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMBINES CSVs ##\n",
    "import os\n",
    "import glob\n",
    "fulldataset = []\n",
    "for fname in glob.glob(directory_path):\n",
    "    fname_as_list = create_list_from_file(fname)\n",
    "    fulldataset = fulldataset + fname_as_list\n",
    "print(len(fulldataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329fb67",
   "metadata": {},
   "source": [
    "=========================================================================================================\n",
    "\n",
    "### __SECTION 4:__ _Counting Unique Tasks_ ###\n",
    "__Note:__\n",
    "- Prints the total number of unique Scheduled Tasks found within the entire, unfiltered dataset\n",
    "- Prints the number of times each Scheduled Task occurs\n",
    "- __Warning:__\n",
    "    - If used on the entire dataset of 12,000+ files, you may have hundreds, if not thousands, of unique scheduled task names\n",
    "    - During testing, using a sample of 2000+ entries returned around ~350 unique tasks\n",
    "- Stored in Python as a _Dictionary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37679d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_unique_tasks(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823fe68",
   "metadata": {},
   "source": [
    "=========================================================================================================\n",
    "\n",
    "### __SECTION 5:__ _Filtering Out Pre-Specified Tasks_ ###\n",
    "__Note:__\n",
    "- No printing / output in this section\n",
    "- _Vanilla_dataset_ contains all entries that have been checked against a list of tasks deemed \"safe\"\n",
    "- _NeedsThreatHunting_Dataset_ contains all entries that still need to be checked for potentially malicious tasks\n",
    "\n",
    "- All tasks that match/contain the \"keyword(s)\" in VanillaTasks will be added to vanilla_dataset\n",
    "- All tasks that DO NOT match/contain the \"keyword(s)\" in VanillaTasks will be added to NeedsThreatHunting_dataset\n",
    "\n",
    "- Stored in Python as a _list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARSING THROUGH COMBINED LIST OF ALL DATAPOINTS FOR SEARCH TERMS ##\n",
    "vanilla_dataset = []  \n",
    "NeedsThreatHunting_dataset = [] \n",
    "\n",
    "VanillaTasks1 = create_list_from_file(file_path)\n",
    "VanillaTasks = []     \n",
    "for eachtask in VanillaTasks1:\n",
    "    VanillaTasks.append(eachtask[0])\n",
    "    \n",
    "for row in fulldataset:\n",
    "    name = row[1]\n",
    "    boolean = VanillaFilter(name)\n",
    "    if boolean == True:\n",
    "        vanilla_dataset.append(row)\n",
    "    elif boolean == False:\n",
    "        NeedsThreatHunting_dataset.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add99919",
   "metadata": {},
   "source": [
    "=========================================================================================================\n",
    "\n",
    "### __SECTION 6:__ _Output / Verification_ ###\n",
    "__Note:__\n",
    "- Prints:\n",
    "    - 'Num. of Tasks Deemed Safe'\n",
    "        - _What's filtered out_\n",
    "    - 'Num. of Tasks Left To Check'\n",
    "        - _TaskNames whose names or keyword could not be found in the VanillaFilter CSV_\n",
    "    - 'Total Number of Tasks Filtered'\n",
    "        - _Self explanatory_\n",
    "    - 'Expected Number of Tasks Filtered'\n",
    "        - _Self explanatory_\n",
    "- If the Total and Expected Number of Tasks Filtered differ in any way, something broke during the sorting process\n",
    "- Stored in Python as an _Integer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUT / VERIFCATION ##        \n",
    "print('Tasks Deemed Safe =', len(vanilla_dataset), '(AKA Vanilla Dataset)')        \n",
    "print('Tasks Left to Check =', len(NeedsThreatHunting_dataset), '(AKA Threat Hunting Dataset)')\n",
    "print('Total =', (len(vanilla_dataset) + len(NeedsThreatHunting_dataset)))\n",
    "print('\\n')\n",
    "print('Expected Total =', len(fulldataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc769c",
   "metadata": {},
   "source": [
    "=========================================================================================================\n",
    "\n",
    "# Additional Notes on Setup:\n",
    "- In Section 2, you are required to input __two__ different paths, one for a directory and one for a file\n",
    "    - The _directory path_ should lead to a folder that contains all CSV files with data to be inspected\n",
    "    - The _file path_ should lead to a singlular CSV file that contains all names or keywords related to Scheduled Tasks data you would like to see filtered out of the dataset, hence the name \"VanillaTasks\"\n",
    "\n",
    "- Setting up the file containing \"Vanilla Tasks\":\n",
    "    - Create a new excel document\n",
    "    - In cell __A1__, add an arbitrary column header like 'Vanilla Tasks to Remove'\n",
    "        - This will not be used to filter, but is required\n",
    "    - In cell __A2, A3, A4, etc__, type _names_ or _keywords_ of tasks you'd like filtered out\n",
    "    - Click \"File\", \"Save as\" \n",
    "        - __File Name:__ _arbitrary_, __ex:__ VanillaTasks.csv\n",
    "        - __File type:__ _'CSV (Comma delimited) (*.csv)'_\n",
    "    \n",
    "- Vanilla Tasks _keyword_ / _name_ Example:\n",
    "    - If TaskName is _'\\Adobe Flash Player Update'_, then the \"keyword(s)\" added to 'Vanilla Tasks' CSV could be:\n",
    "        - _'Adobe Flash Player Update'_   <--- __Most__ exact, less likely to filter other tasks by accident\n",
    "        - _'Adobe'_                       <--- __Least__ exact, more likely to filter other tasks by accident\n",
    "\n",
    "- Setting up the directory containg the 12,000+ CSVs:\n",
    "    - Two, very simple steps:\n",
    "        - Throw every CSV file you want to work with into a single folder\n",
    "        - Copy and paste the file path of this folder into its designated spot within this script (Refer to Section 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1afb27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
